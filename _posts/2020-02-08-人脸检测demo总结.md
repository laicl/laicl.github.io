---
layout:     post
title:      多路人脸检测demo显示优化总结
subtitle:   基于hw芯片，多路人脸Demo显示优化总结
date:       2020-02-08
author:     LCL
header-img: img/post-bg-molly.jpg
catalog: true
tags:
    - 人脸检测
    - 多路处理
    - atlas
typora-root-url: ..
---

#### 写在开始的话
还没想好这篇博客要表达的重点是什么，暂时先想到哪儿写到哪儿。争取把这个开发过程中，通用的一些性能分析和调优方法讲清楚。

#### 介绍
做这个demo的初衷就是尽可能地发挥硬件的能力，用一块芯片完成12路甚至16路视频的人脸检测。所以，在模型的人脸检测精度达标的情况下，处理速度越快越好，CPU占用率越低越好。（这是一句正确的废话）
Talk is cheap, show me the code. 我是基于[Atlas FaceDemo](https://gitee.com/HuaweiAtlas/FaceRecognition)进行修改的。Atlas FaceDemo里模块很多，包含了拉流、检测、跟踪、识别、结果展示等等。我运行人脸Demo，发现[结果拼接](https://gitee.com/HuaweiAtlas/FaceRecognition/blob/master/src/host/cpp/StreamDataOutputEngine.cpp)模块CPU占用率比较高。我手头上只有一个48核服务器。要运行4个进程，每个进程各完成12路的人脸检测，服务器压力比较大。所以，要尽可能地把cpu占用率降下来。

#### 业务流程
为了方便理解后续要优化的功能逻辑。我们了解下多路人脸检测的简单流程。
![flowchart]({{site.baseurl}}/img/face_demo_flowchart1.jpg)
1、ffmpeg拉流，使用ffmpeg从12路视频流获取AVPacket视频帧数据。
2、视频帧解码，使用芯片的硬件解码模块对视频帧进行解码，也可以使用opencv软件解码。此时，解码的结果是按yuv格式排布的图像数据。
3、对图像进行人脸检测。因为demo使用的人脸检测模型的输入数据要求是rgb。在模型推理之前，需要使用芯片自带的色域转换，将解码得到的yuv数据转换为rgb。再将rgb数据送入模型，进行推理。demo推理结果是包含6个元素的一维向量，分别是目标类别label_id、置信度confidence、目标框中心坐标(x,y)、目标框的宽高(w,h)。
4、结果拼接。使用opencv把检测模型的结果框画到图像上，并且把12个图像拼成12宫格，展示在一个1920x1080的图像上。这个模块的输入包含两部分，一是视频帧解码得到的yuv图像数据，另一个是模型检测得到的目标数据（6个元素的向量）。但是在调用opencv接口进行画框和图像拼接的时候，需要使用rgb的图像数据。所以，这里会涉及到四个步骤：a) yuv转rgb格式；b) rgb图像画框；c) rgb图像拼接；d) rgb转yuv格式。最后步骤d的rgb转yuv格式，是因为在流程5中要把拼接结果推送到对应的ip进行展示。
5、ffmpeg推流。把1920x1080的大图像数据编码为h.264，并推送到目标ip地址。这样，就可以利用视频播放软件打开对应的rtsp流，查看结果。

